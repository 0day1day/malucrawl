
\section{ClamAV Malware Scan}
In order to fully identify the existence of malware inside a webpage, a 
signiture based malware detection method must be implemented in our system. 
The most obvious approach is to add anti-virus software to the scanning system 
to enable the classification of URLs and files with non-HTML contents, such 
as binary files and scripts. Therefore we decide to use ClamAV as a step in 
scanning for this purpose, which is also a low interaction malware detection 
method. 

\subsection{Design}
\subsubsection{Introduction to ClamAV}
ClamAV is an open source anti-virus software developed for UNIX, and is also 
available to Linux systems. It has a command-line scanner with open access 
signatures database, where the anti-virus engine is also in a shared library. 
The software provides both on-demand and on-access scanning on Linux systems, 
where on-access scanning is achieved with a scalable multi-threaded daemon. 
It also has a wide support for various file formats, including almost all 
popular compressed files, emails and documents. \\
A report in 2011 regarding the overall rating of ClamAV states that it has a 
ranking of 12 among all public released anti-virus 
applications,\cite{shandowserver} where the most impressive part is its fast 
response time and scanning speed. We believe this is a key feature we should 
make use of due to the purpose of our system. The processing of our large URL
datasets requires considerable computing power, where the scanning process 
occupies most of it. \\
ClamAV also has a third party python API known as pyClamAV. It is a free 
library implemented in C and binds to the ClamAV's own API, libClamAV. However 
the newest version moved its focus to pyClamd, which is an interface to Clamav 
daemon instead of libClamAV, due to latest updates in ClamAV removed some key 
APIs in libClamAV. \\
Overall, the conclusion we made is to use ClamAV instead of any other 
anti-virus software due to following reasons: 
\begin{itemize}
\item Open source.
\item Large signiture database and high detection rate.
\item Linux compatible. 
\item Python compatible. 
\item Wide support for various file types. 
\item Fast response time and scanning speed. 
\end{itemize}

\subsubsection{ClamAV with HTML crawler}
We build a program to download URLs and perform static analysis, which 
conforms to the term --- low interaction. The given URL should always contain 
a HTML file, and after virus scan the program crawls it in order to extract links to pages 
under the website. After that the extracted links are again downloaded and 
scanned with ClamAV. This time the file types are uncertain, therefore we give 
the program a max file size to download as well as a time limit in case of 
slow or broken connection. This is a vital step to avoid huge files such as 
high definition videos as well as those does not give file length, for example, an 
infinite length radio stream. \\
We ignore links to other domains, for the reason that only the investigation 
of the given URL's domain is concerned. Duplicate links are removed, and the 
program avoids downloading the same page twice or more, as the root page needs 
to be processed twice: once for crawling and once for virus scan. \\
Every ClamAV scan is performed by a separate Celery task, which means the 
number of tasks should be $n+1$ where n is the number of links interested and 
the other time is for the root page.  

\subsection{Implementation}
\subsubsection{Libraries}
The following libraries are used in this Python application:
\begin{itemize}
\item {\bf pyClamd}\\This is the ClamAV daemon library used for URL content 
scanning. Compared to ordinary Clamav, a daemon is faster and 
multi-threaded. It listens on Unix local socket or TCP socket to work, which 
means it supports remote scanning. However in our system only local socket is 
used as remote scanning is not necessary. 
\item {\bf requests}\\The network library used for HTTP enqueries. In this 
program it is used for downloading the root HTML page and links inside of it. 
\item {\bf lxml.html}\\lxml is a XML toolkit work for Python, it creates a 
binding to the C libraries libxml2 and libxslt.\cite{lxml} lxml.html is a 
package specifically designed for HTML parsing, it acts as a HTML crawler 
which extracts links. 
\item {\bf eventlet.timeout}\\Eventlet is a Python library that enables 
concurrent networking. The $timeout$ package provides an universal timeout 
solution achieved with green threads, it is this package makes the time limit 
for downloads realisable. 
\end{itemize}
\subsubsection{Program implementation}
The program flow is explained as following:
\begin{enumerate}
\item A Celery task is created with a URL, which is downloaded via requests. 
\item the HTML parser parses the downloaded content as a HTML file, then makes 
all links in it absolute, which is achieved with the base URL provided. 
\item All links are extracted with the library function html.iterlinks(), 
results are returned as a list for easy manipulation. Thus duplicate links and 
those with different domain names are removed.
\item For each link we perform the download function with a separate Celery 
task. A get request which only 
aquires HTTP header is sent, such that the file size can be determined by the 
{\em content-length} field. If the file length is within limit, just before 
the download begins a green thread with {\em Timeout} is created which throw
an exception if download time exceeds. Zero is returned if any exception 
occurs before or during downloading, which includes but not limited to request 
exceptions (such as error response code) and time outs. After successful 
downloading a clamd scan to the content is called, the return value should be 
either {\em None} or virus detected message. 
\item The final list of malware detected is merged between the results 
returned from all link downloads and the scan result of the root page, which 
returns to the main system.  
\end{enumerate}
