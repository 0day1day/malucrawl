\paragraph{Trend Analysis}
Each trend analysis phase should be able to query some external services and determine a set of keywords. These keywords can then be returned, to be expanded to URLs by the URL Discovery phase.

Libraries available that will be useful include:

\begin{itemize}
    \item requests A simple API for making HTTP requests
    \item feedparser For parsing RSS feeds
\end{itemize}

Trend sources to be used are Twitter Trending Topics API, RSS Feeds and Trends from the Sun JSON Feed.



It is important to note that further possibilities for keyword determination should be researched. The sources suggested here are the ones I think will be the easiest to interface to, but might not be the sources malware distribution networks actually use!

\paragraph{Twitter}
The twitter api is available in multiple versions, 1 and 1.1. Currently 1 is deprecated and it is recommended that new applications build against 1.1.  The disadvantage of the 1.1 API is that it requires a ``User Context'', ie all requests must be authenticated based on Application-User key.

The Twitter API provides access to global trending topics at

\verb`https://api.twitter.com/1.1/trends/place.json`

Documented at \verb`https://dev.twitter.com/docs/api/1.1/get/trends/place`

The keyword information is available at [0].trends[].name.

Topics with a \# infront are Twitter ``hashtags'', and are usually camel cased. The system should be able to convert from camel cased to space delimited. As in \verb`http://commons.apache.org/lang/api-3.0.1/org/apache/commons/lang3/StringUtils.html#splitByCharacterTypeCamelCase%28java.lang.String%29`

Eg. \#BrazilWantsOneDirection -> Brazil Wants One Direction
\#XFactorPHFinale -> X Factor PH Final

In the case of all capitals eg NIALLSAYHITOTURKEY, it's probably best to drop it.

\subsubsection{RSS}
A set of RSS feeds should be collected and parsed, ``feedparser'' is likely to be useful for this.

Each item from a feed should be entered into a corpus, to calculate keywords from items in the corpus algorithms such as TFIDF should be used. The natural language library ``metanl'' is likely to be helpful.

Note that each feed can be processed separately. Ie one corpus per feed.

\subsubsection{A note on the Sun}
The Sun provides a JSON API for the top 200 most commented on articles at \verb`http://bootstrap.thesun.fyre.co/api/cust/ni/todays_hottest/200.json`. In most cases the content of this JSON file can be treated exactly the same as an RSS feed however care should be taken to deal with the dodgy encoding of some of the returned headlines: ``Terry a â\\u0080\\u0098liarâu\\0080\\u0099'' should be ``Terry a \"liar\"''.

\subsubsection{Even More Sources}
Examples include:
\begin{itemize}
    \item Link aggregation sites such as Reddit and Digg.
    \item Search engine common searches and trends such as Google, Bing and Yahoo trends services.
    \item Network logs such as the campus search trends. It should be noted that this data is very unlikely to be accessible by malware distribution networks and might allow us to see trends before they do.
\end{itemize}

%implementation
\subsection{Implementation}
The most popular Twitter library for Python is called Tweepy, which is also 
our choice. This library provides access to Twitter version 1.1 API, which 
requires OAuth authentication. Therefore we created a Twitter account 
dedicated for this program, which provides consumer key and access token in 
order to pass the authentication. \\
The program then retrieves global trending keywords from the Twitter's {\em 
trends/place} API. As previous described, in order to cope with keywords with 
hashtags, we implemented a hashtag handler, who's role is to remove the hash 
tag and convert the camel case word to sentence case. This is achieved by 
applying regular expression substitution: \\
\newline
\verb`re.sub(r'((?<=[a-z])[A-Z]|(?<!\A)[A-Z](?=[a-z]))',r' \1',camelCaseString)`
\newline
Before the conversion happens the keyword is also decoded into unicode to 
avoid dodgy encodings. 
\paragraph{}
We also hard coded a list of famous news websites' RSS feed addresses in the 
program, the website list include BBC, NewYork Times, Guardian, The Washington 
Post and so on. Our RSS parser downloads every feeds, parses them into list 
then extracts news titles in them. Characters require encodings other than 
UTF-8 are ignored. The downloading is achieved with requests and any network 
errors will be handled gracefully. 
\paragraph{}
The top 200 commented articles from Sun JSON API are handled in a similar way 
as that of RSS feeds. Due to good compatibility between Python and JSON we 
were able to extract article titles without any difficulties. 
\paragraph{}
Twitter trends, RSS feeds and Sun JSON are implemented as three different 
functions which are going to be run as separate Celery tasks. All of them 
return a list of keywords or sentences to the main system. 
