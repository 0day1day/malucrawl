\subsection{Design}
The project can be split into four phases of operation, Trend Source Analysis, Searching for Keywords, Scanning URLs and Storing those results in a database.

This section of the report describes the design of a framework that connects those phases of operation together in a performant manner.

\subsubsection{A Distributed System}
To be able to operate quickly the system can be organized into a parallel architecture.

This is due to the independent nature of many components of the system: Trend Source Analysis, Searching for Keywords and Scanning URLs.

Each trend source is not dependent on on any other trend source, however the search engine tasks cannot be dispatched without a keyword from the Trend Analysis phase to be able to operate.

The process of scanning and determining if a URL has at one point returned a malicious page or not is entirely independent of both any other URL or any other particular method of scanning.

As such multiple sources can be queried for trends and those trends can then be used to browse for URLs using search engines, while multiple malware scanners are able to operate on multiple discovered URLs at the same time.

\subsubsection{}

Keyword Finding and Trend Analysis are both simpler phases and effect how data flows through the system and as such are discussed together in this section.

\subsubsection{Keyword Finding}
Each ``keyword finding'' task should be able to query some external services and determine a set of keywords. These keywords can then be dispatched to other tasks.

Libraries available from PyPi likely to help with this include:

\begin{itemize}
    \item requests A simple API for making HTTP requests
    \item feedparser For parsing RSS feeds
    \item lxml.html and BeatuifulSoup. For parsing and interacting with ``real world'' HTML
\end{itemize}

It is important to note that further possibilities for keyword determination should be researched. The sources suggested here are the ones I think will be the easiest to interface to, but might not be the sources malware distribution networks actually use!

\subsection{Implementation}
This section describes how to use a set of workers to determine trending topics and sites relating to those topics

Each result from the ``keyword finding'' task should result in a new ``search engine'' task.
Each result from the ``search engine'' task should result in a new ``malware finding placeholder'' task
Each result from the ``malware finding placeholder'' should be printed to STDOUT by the master node.
% python, pip and PyPi
% HTML/XML, RSS and JSON parsing
% OAuth2
% Celery basics
\subsubsection{Tools}
\subsubsubsection{Celery}
Celery is a ``Distributed Task Queue'' that allows standard python functions to be converted into Tasks that can operate asynchronously and in parallel on a large cluster of machines.

These Tasks can be dispatched in real-time or can be scheduled to take place at a particular time in the future, or at a predictable time intervals.  This scheduling feature particularly useful for this project because it is required to process a complete flow of operation through all phases each day.

Celery supports multiple threading models including multiprocessing and Eventlet.

multiprocessing, part of the Python Standard Library, is a threading model that spawns separate processes in which to consume execution units.  This means that each thread is very resource intensive because it is a full process and requires all of the code and a complete instance of the Python environment.

Eventlet is a wrapper around either epoll or libevent both of which provide a ``mechanism to execute a callback function when a specific event occurs on a file descriptor'' such as a notification on receiving data in a TCP socket.  Potentially thousands of ``green'' threads can be spawned with minimal resource usage, but any blocking operation causes the entire pool to block also. This is more useful than the multiprocessing threading model for most phases of the system as the majority of them spend most of their execution time downloading data from the Internet.

To be able to communicate to multiple workers operating on multiple, disparate and networked machines Celery requires a message broker and a result back end to make results of Tasks available to other Tasks.  The recommended message broker is RabbitMQ and the recommended result back-end is Redis.

\subsubsubsection{RabbitMQ}
Although being one of many possible message brokers, RabbitMQ was chosen as the message broker not just because it is the system recommended by Celery.

Celery supports a range of message broker systems, two message queue systems: RabbitMQ, and Beanstalk, three NoSQL databases: Redis, MongoDB and CouchDB and any SQL database supported by SQLAlchamey or the Django ORM.

While some of the database systems support publish and subscribe on records none of them are capable of managing multiple queues of messages on widely distributed machines.  The SQL databases are to be particularly avoided and only used for test or because no other system is available. This is known as the ``Database as Queue Anti-pattern''\cite{database-as-mq}.

Using a database as a queue is an anti-pattern because the only way to get notified of messages is to repeatedly poll the database and also because a database is difficult to share among multiple client machines while remaining synchronized.

The only remaining systems after the others have been eliminated are Beanstalk and RabbitMQ, both message queue systems.  RabbitMQ was chosen over Beanstalk because it is easier to distribute the queue itself over a network of machines.

\subsubsubsection{Redis}

\subsubsubsection{Python}

\subsubsubsection{Django ORM}


\subsubsubsection{Future Extension}
The system should be designed so that it is possible to avoid repeating the ``same'' task too often. Eg a ``search engine'' task for the same keyword, or ``malware finding'' task for the same URL should only be repeated once in any given time period.

To implement this a set of keywords and URLs should be maintained. Each item should be dropped from the set after a certain time. This could be in a database eg. the existing Redis instance used for the result backend:

\verb/{"task:(keyword|url):#{data}":{"result":taskid, "requested":time_stamp}}/

The simplest solution is that: each time a task is called it checks in Redis for its task key, eg ``task:keyword:foo'' and returns the result at ``taskid'' if the time is recent enough. Otherwise it adds itself to Redis and returns normally.

Despite being conceptually to check the sets each time, this is a waste of bandwidth. As such the controlling node should be able to check in most cases.

Note: In the end it might not be worth it, as we will only be checking the keyword sources as often as they claim to update and checking for duplicate scanned URLs could be done later in the stack eg. before they are dispatched to the Windows vms.