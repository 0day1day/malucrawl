\subsection{Literature review}
The research in background knowledges gave us better understanding of current 
technologies about malware distribution over the internet. We realised the 
most used approach is to poison search engines which is called Search Engine 
Optimization (SEO), and in order to achieve this the use of trending terms 
is necessary. The processes of a SEO attack was explained, and a method to 
track down SEO campaigns was also introduced. We looked into details about 
how trending keywords can be applied to generate MFA and malicious websites 
such that they can be found by search engines. The significance of search 
engine intervention was proved by Google during Februrary, 2011. \\
Another report acts as our basics of low and high interaction client 
honeypots. It explained passive and active methods for malware detection 
over the web as well as the differences between low and high interaction 
approaches. The automatic malware collecting system they produced inspired 
us the details about how to implement our system. 

\subsection{Wine Explorer and ClamAV scanner}
Wine Explorer provides a lightweight solution for high interaction malware 
detection. It has advantages over virtual machines and emulators including 
short reset time, fast execution and RAM/disk saving, where the potential 
security threat blocks its way to perfection. The program is also approved to 
execute stably, for instance the correctness of the Wine prefix package is 
guaranteed via hash check which avoids external modifications of it. We 
explained the lacking of complete detection as well, which could be 
implemented if we have more time in this project. 
\paragraph{}
We also provide a signiture-based malware scanning powered by ClamAV. The 
HTML crawler in the same program gives links for ClamAV to scan and an 
overall list of malicious links in a webpage can be produced from the 
application. It has fast scanning and executes concurrently which rises 
the system's throughput to a significant amount where disadvantages exists 
such as unprecise link filtering. An improvement could be a deeper level of 
website crawling, and better link processing which requires further amount 
of time to complete. 
