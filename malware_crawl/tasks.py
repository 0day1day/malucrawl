import tweepy
import requests
import re
import lxml.html
import feedparser
from urlparse import urlparse, parse_qs
import jsonschema
import json
import pkgutil

from celery import task, group, chain

from .scan.alexa import alexa_malware_scan
from .scan.web_api import wot_malware_scan
from .scan.manual_html import manual_html_malware_scan


consumer_key = "8MJXt6b7JO4D9CQuDdzNLg"
consumer_secret = "pJuG88umFd61hVmqgcv4S1A26B9FROwr1w2nPLBjMHk"

access_token = "877860914-fwOaxBigey11rhzETtE5fh5djzvzxFmIDWFVifAi"
access_token_secret = "Pv9j7PhpSktS7OFokWiV84T3GeYJeSSrZsJKrj1O4"

feeds = (
    'http://newsrss.bbc.co.uk/rss/newsonline_world_edition/front_page/rss.xml',
    'http://www.nytimes.com/services/xml/rss/nyt/HomePage.xml',
    'http://www.guardian.co.uk/rssfeed/0,,1,00.xml',
    'http://newsrss.bbc.co.uk/rss/newsonline_world_edition/front_page/rss.xml',
    'http://www.washingtonpost.com/wp-dyn/rss/print/asection/index.xml',
    'http://www.huffingtonpost.com/thenewswire/full_rss.rdf',
    'http://www.npr.org/rss/rss.php?id=2',
    'http://online.wsj.com/xml/rss/3_7077.xml'
)

validator = jsonschema.Draft3Validator(json.loads(pkgutil.get_data("malware_crawl", "malware_discovery_schema.json")))


def to_unicode(obj, encoding='utf-8'):
    # http://farmdev.com/talks/unicode/
    if isinstance(obj, basestring):
        if not isinstance(obj, unicode):
            obj = unicode(obj, encoding)
    return obj


def camelCaseToSentenceCase(string):
    # http://stackoverflow.com/a/9283563
    return re.sub(r'((?<=[a-z])[A-Z]|(?<!\A)[A-Z](?=[a-z]))', r' \1', string)


def rssListParse(urlList):
    def rssParse(url):
        response = requests.get(url)
        if(response.status_code != requests.codes.ok):
            #print 'Error when downloading rss feed with url: '+url
            return []
        feed = feedparser.parse(response.text)
        list = []
        for e in feed.entries:
            list.append(to_unicode(e.title))
        return list
    titleList = []
    for url in urlList:
        l = rssParse(url)
        titleList = titleList + l
    return titleList


@task
def sun_trends():
    response = requests.get(
        'http://bootstrap.thesun.fyre.co/api/cust/ni/todays_hottest/200.json'
    )
    return [item["headline"] for item in response.json['data']['conv_list']]


@task
def find_trends():
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)

    api = tweepy.API(auth)

    def hash_tag_handle(name):
        name = to_unicode(name)
        if name.startswith('#'):
            name = camelCaseToSentenceCase(name[1:])
        return name

    trends = (item["name"] for item in api.trends_location('1')[0]['trends'])

    return map(hash_tag_handle, trends)


@task
def search(keyword):

    def dogpile_link_handle(url):
        # parse the click handler from dogpile to get the real URL
        return parse_qs(urlparse(url).query)["du"][0]

    response = requests.get(
        "http://www.dogpile.co.uk/search/web",
        params={"q": keyword}
    )

    links = (
        anchor.get("href") for anchor in lxml.html.fromstring(
            response.content,
            base_url=response.url
        ).cssselect(".webResult .resultDisplayUrl")
    )

    return map(dogpile_link_handle, links)


@task
def malware_scan(url):
    print url
    try:
        response = requests.get(url, timeout=10.0)
        e_title = lxml.html.fromstring(
            response.content,
            base_url=response.url
        ).cssselect("title")
        if e_title:
            return e_title[0].text
    except:
        return "Failed"


def complete_crawl():
    source = "twitter"
    find_trends.apply_async(
        link=begin_search.subtask(args=(source,))
    )


@task
def begin_search(keywords, source):
    for keyword in keywords:
        search.apply_async(
            args=(keyword,), link=begin_scan.subtask(args=(keyword, source))
        )


@task
def begin_scan(urls, keyword, source):
    for url in urls:
        malware_scan.apply_async(
            args=(url,), link=begin_store.subtask(args=("fake_scan", url, keyword, source))
        )


@task
def begin_store(opinion, scanner, url, keyword, source):
    print (opinion, scanner, url, keyword, source)

from models import (TopicSet, Topic, Search, Result, MalwareReport, Opinion)


def store(j_report, commit=True):
    try:
        validator.validate(j_report)
    except jsonschema.ValidationError:
        raise ValueError("Not a valid report")

    report = TopicSet(
        source=j_report["source"],
        discovered=j_report["discovered"]
    )
    topic_set = []
    for j_topic in j_report["topics"]:
        topic = Topic(
            keyword=j_topic["keyword"]
        )
        topic_set.append(topic)
        search_set = []
        for j_search in j_topic["searches"]:
            search = Search(
                source=j_search["source"],
                discovered=j_search["discovered"]
            )
            search_set.append(search)
            result_set = []
            for j_result in j_search["results"]:
                result = Result(
                    url=j_result["url"]
                )
                result_set.append(result)
                malware_report_set = []
                for j_malware_report in j_result["malware_reports"]:
                    malware_report = MalwareReport(
                        reporter=j_malware_report["reporter"]
                    )
                    malware_report_set.append(malware_report)
                    opinion_set = []
                    for j_opinion in j_malware_report["opinions"]:
                        opinion = Opinion(
                            type=j_opinion["type"],
                            confidence=j_opinion["confidence"]
                        )
                        opinion_set.append(opinion)
                    malware_report.opinion_set.add(*opinion_set)
                result.malware_report_set.add(*malware_report_set)
            search.result_set.add(*result_set)
        topic.search_set.add(*search_set)
    report.topic_set.add(*topic_set)

    if commit:
        report.save()
    return report
