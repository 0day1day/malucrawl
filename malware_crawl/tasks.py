import tweepy
import requests
import re
import lxml.html
import cssselect
from urlparse import urlparse, parse_qs, urlunparse
from urllib import urlencode


from celery import task, group

consumer_key = "8MJXt6b7JO4D9CQuDdzNLg"
consumer_secret = "pJuG88umFd61hVmqgcv4S1A26B9FROwr1w2nPLBjMHk"

access_token = "877860914-fwOaxBigey11rhzETtE5fh5djzvzxFmIDWFVifAi"
access_token_secret = "Pv9j7PhpSktS7OFokWiV84T3GeYJeSSrZsJKrj1O4"

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)


def to_unicode(obj, encoding='utf-8'):
    # http://farmdev.com/talks/unicode/
    if isinstance(obj,basestring):
        if not isinstance(obj, unicode):
            obj = unicode(obj,encoding)
    return obj

def camelCaseToSentenceCase(string):
    # http://stackoverflow.com/a/9283563
    return re.sub(r'((?<=[a-z])[A-Z]|(?<!\A)[A-Z](?=[a-z]))', r' \1', string)

@task
def find_trends():
        
    def hash_tag_handle(name):
        name = to_unicode(name)
        if name.startswith('#'):
            name = camelCaseToSentenceCase(name[1:])
        return name
    
    trends = (item["name"] for item in api.trends_location('1')[0]['trends'])

    return map(hash_tag_handle,trends)

@task	
def search(keyword):
    base_url = urlunparse(("http","www.dogpile.co.uk","/search/web","",urlencode({"q":keyword.encode("utf-8")}),""))
    
    def dogpile_link_handle(url):
        # parse the click handler from dogpile to get the real URL
        return parse_qs(urlparse(url).query)["du"][0]
    

    search_results = requests.get(base_url).content
    links = (anchor.get("href") for anchor in lxml.html.fromstring(search_results, base_url = base_url).cssselect(".webResult .resultDisplayUrl"))

    return map(dogpile_link_handle, links)

@task
def malware_scan(url):
    e_title = lxml.html.fromstring(
        requests.get(url, timeout=10.0).content,
        base_url = url
    ).cssselect("title")
    if e_title:
        return e_title[0].text

def complete_crawl():
    trends = find_trends.delay().get()
    
    for search_results in group((search.s(trend) for trend in trends)).delay().iterate():
        yield group((malware_scan.s(url) for url in search_results)).delay().join()
