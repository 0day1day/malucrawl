import requests
import zipfile
import cStringIO as StringIO
import csv
import redis
from urlparse import urlparse
from publicprefix import PublicPrefixList

redis_db = {
    "slave": redis.Redis(
        unix_socket_path="/var/run/redis/redis.sock",
        db=0,
        password="Km7icdOpKvb6JIzN40iG"
    ),
    "master": redis.Redis.from_url("redis://:Km7icdOpKvb6JIzN40iG@malucrawl.ecs.soton.ac.uk/0")
}


def alexa_top_million():
    """Returns an iterable of objects describing the top million domains from Alexa
    It turns out this list can be used as a set and held in memory fairly
    easily:

    domains = frozenset({item["domain"] for item in alexa_top_million()})
    if "google.com" in domains:
        return u"safe"

    However it might be better to stick this in a *real* database local to
    each worker, and update it each day. Might as well use the Redis DB we
    have lying around. It might be even better to have a BloomFilter before
    even bothering to check the DB.
    """
    return csv.DictReader(
        zipfile.ZipFile(
            StringIO.StringIO(
                requests.get("http://s3.amazonaws.com/alexa-static/top-1m.csv.zip").content
            )
        ).open("top-1m.csv"),
        fieldnames=("score", "domain")
    )


def update_alexa():
    pipe = redis_db["master"].pipeline()
    pipe.delete("alexa:domains")
    pipe.sadd("alexa:domains", *alexa_top_million())
    pipe.execute()


def alexa_malware_scan(url):
    domain = PublicPrefixList().get_public_suffix(urlparse(url).netloc)  # IRIs are going to be a pain here.
    if not redis_db["slave"].sismember("alexa:domains", domain):
        return "malware"
