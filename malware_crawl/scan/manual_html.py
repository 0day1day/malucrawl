from __future__ import print_function
import requests
import lxml.html
import urlparse

_malware_keywords = {
    'Queen Elizabeth II': 0
}


def check_content(url):
    global _malware_keywords
    try:
        response = requests.get(url, timeout=10.0)
        content = response.content.lower()
        for k, v in _malware_keywords.items():
            _malware_keywords[k] = _malware_keywords[k] + content.count(k)
        return _malware_keywords
    except:
        #raise
        return 'Could not check content of: ' + url


def find_inner_urls(url):
    try:
        response = requests.get(url, timeout=10.0)
        html = lxml.html.fromstring(
            response.content.lower(),
            base_url=response.url
        )
        parsed_url = urlparse.urlparse(response.url)
        base_url = urlparse.urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', ''))
        urls = html.xpath('//a/@href')
        for i, u in enumerate(urls):
            if not u.startswith('http://'):
                urls[i] = base_url + u
        return urls
    except:
        #raise
        return 'Could not find urls for: ' + url


def malware_scan(url):
    global _malware_keywords
    try:
        urls = find_inner_urls(url)
        for i, u in enumerate(urls):
            print('checking url of: ' + u, str(i) + '/' + str(len(urls)))
            check_content(u)
        return _malware_keywords
        #e_title = lxml.html.fromstring(
            #response.content,
            #base_url=response.url
        #).cssselect("title")
        #if e_title:
            #return e_title[0].text
    except:
        raise


if __name__ == '__main__':
    url = 'http://uk.movies.yahoo.com/person/emma-watson/'
    print(malware_scan(url))
