from __future__ import print_function, unicode_literals
import requests
import lxml.html
import six
from progress.bar import Bar
from malware_crawl.models import Malicouse_sites, Keywords



def get_malware_keywords():
    """
        Read the list of keywords from the database and return
        a dictionary of malwares in the format of:
        _malware_keywords = {'Queen Elizabeth II':0,}
    """
    kw = Keywords.objects.all()
    malware_keywords = {}
    for m in kw:
        mal_dict[m.word.lower()] = 0 
    return mal_dict


def prepare_malware_keywords(malware_keywords):
    """
        Turns the malware_keywords to lower case and return 
        the keywords dictionary so it is case insensitive

        args
        malware_keywords -- the keywords of malwares to the number
                    of times they appear
    """
    _lower_mk = {}
    for k,v in malware_keywords.items():
        _lower_mk[k.lower()] = v;
    return _lower_mk


def check_content(url, malware_keywords):
    try:
        response = requests.get(url, timeout=10.0)
        response.raise_for_status()
    except requests.exceptions.HTTPError:
        return {}

    text = response.text.lower()
    return {keyword: text.count(keyword) for keyword in malware_keywords} 


def malware_scan(url, malware_keywords):
    """
    Crawls a page depth 1, returns the total count for each of 
    the keywords in malware_keywords for each of the pages in the crawl.
    """

    response = requests.get(url, timeout=10.0)
    html = lxml.html.fromstring(
        response.content,
        base_url=response.url
    )
    html.make_links_absolute(resolve_base_href=True)

    keyword_counts = dict.fromkeys(malware_keywords, 0)

    for url in Bar().iter({link for element, attribute, link, pos in html.iterlinks()}):
        for keyword, count in six.iteritems(check_content(url, malware_keywords)):
            keyword_counts[keyword] += count

    if sum(six.itervalues(keyword_counts)) > 20:
        return True
    else:
        return False


def save_to_db(url, malicious):
    """
        Given a url and if it is malicious or not, we save
        the url and the maliciousness of it into the database
    """
    site = Malicouse_sites(url=url, malicious=malicious)
    site.save()


def scan_save (url, malware_keywords):
    """
        First scanning the url given then based on the results
        we save it to the database
    """
    is_mal = malware_scan(url, malware_keywords)
    save_to_db(url, is_mal)

def manual_html_malware_scan(urls):
    """
        Given a url, reads the page and the content of urls inside the given 
        set of urls; then based on the malware_keywords, it would classify
        the urls as malicious or not.

        urls -- an array of urls which are going to be identified as malicious
    """
    for item in urls:
        item[opinion].append({"type": "placeholder", "confidence": 1.0})
    return urls

if __name__ == '__main__':
    url = 'http://uk.movies.yahoo.com/person/emma-watson/'
    print(malware_scan(url, get_malware_keywords()))
    scan_save(url, get_malware_keywords())