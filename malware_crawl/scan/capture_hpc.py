from datetime import timedelta
import time

from six.moves import zip, map

from celery import task, current_app
from celery.contrib.batches import Batches

from malware_crawl.defaults import redis_db
from django.conf import settings

import cPickle as pickle
from malware_crawl.models import CaptureUrl
from malware_crawl.scan.celery_rpc_client import RevertRpcClient

self_confidence = 0.8

#class CaptureRouter(object):
#
#    def route_for_task(self, task, args=None, kwargs=None):
#        if task == 'malware_crawl.scan.capture_hpc.chpc_malware_scan':
#            return {'queue': 'capturehpc',
#                'routing_key': 'capturehpc.scan'}
#        return None


def redis_cache(key_prefix, prepare, request, items):
    items = list(map(prepare, items))
    unique_items = list(set(items))
    total_response = {}

    # get all the items that are already in cache
    #pipe = redis_db["slave"].pipeline()
    #for item in unique_items:
    #    pipe.get(key_prefix.format(item=item))

    cache_misses = set()
    # add those items we already know about to the total_response
    #for item, cache in zip(unique_items, pipe.execute()):
    #    if cache is not None:
    #        total_response[item] = pickle.loads(cache)
    #    else:
    #        cache_misses.add(item)
    cache_misses = set(unique_items)

    # use the given function to collect the missing values
    total_response.update(request(cache_misses))

    # add to the cache the result from the INTERNET
    pipe = redis_db["master"].pipeline()
    cache_length = timedelta(days=1).seconds
    for item in cache_misses:
        pipe.setex(key_prefix.format(item=item),
            pickle.dumps(total_response[item], -1),
            cache_length
        )
    pipe.execute()

    return [total_response[item] for item in items]


@task(base=Batches, flush_every=500, flush_interval=((timedelta(minutes=30).seconds, 0.1)[settings.DEBUG]))
def chpc_malware_scan(requests):
    sig = lambda url: url

    print requests

    reponses = chpc_malware_scan_real(
        (sig(*request.args, **request.kwargs) for request in requests)
    )
    # use mark_as_done to manually return response data
    for response, request in zip(reponses, requests):
        current_app.backend.mark_as_done(request.id, response)


def chpc_malware_scan_real(urls):
    def prepare(url):
        return url

    def request(urls):
        total_response = {}
        if not urls:
            return total_response

        CaptureUrl.objects.using('capture').all().delete()
        #TODO: do things and return url=>confidence dict
        for u in urls:
            dbu = CaptureUrl(url=u)
            dbu.save(using="capture")
        #call rpc here!
        print("calling start rpc...")
        revert_rpc = RevertRpcClient()

        r = revert_rpc.call("start")

        #test to see if all the urls are processed
        ding = False

        while not ding:
            ding = True
            for url in CaptureUrl.objects.using('capture').all():
                if not url.currentstatus:
                    ding = False
                elif "B" in url.currentstatus:
                    total_response[url.url] = 0.0
                elif "M" in url.currentstatus:
                    total_response[url.url] = 1.0
                elif "E" in url.currentstatus:
                    total_response[url.url] = 0.5

            time.sleep(10)

        print("calling stop rpc...")
        r = revert_rpc.call("stop")

        #print total_response
        return total_response

    responses = redis_cache(
        key_prefix="malucrawl:capture_hpc:{item}",
        prepare=prepare,
        request=request,
        items=urls
    )

    return [{
        "type": "generic",
        "confidence": response
    } for response in responses]


if __name__ == "__main__":
    print chpc_malware_scan_real(("http://google.com", "http://yahoo.com"))
