import clamd
import itertools
import lxml.html
import lxml.html.soupparser
from urlparse import urlparse
import urllib2

from celery import task, chord, group, chain

import requests

urls = (
    "https://www.google.com/intl/en_ALL/images/logo.gif",
    "http://python.org/images/python-logo.gif",
    "http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif",
    "http://listen.technobase.fm/tunein-aacplus-pls"
)

import eventlet
from eventlet.green import os
from datetime import timedelta
from six import binary_type
from six import BytesIO

MAX_SIZE = 1000 * 1024
CHUNK = 1024


headers = {
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30; InfoPath.2; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; MS-RTC LM 8; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)",
}


def fetch(url):
    body = str()
    #body = binary_type()
    timeout = eventlet.Timeout(timedelta(minutes=1).seconds)
    goturl = ""
    try:
        #print os.environ.get("malucrawl_foo", "no changes")
        #os.environ["malucrawl_foo"] = "changed"
        u = urlparse(url)

        if "ftp" in u.scheme:
            response = urllib2.urlopen(url)
            body = response.read()
            goturl = response.geturl()
        else:
            r = requests.get(url, stream=True, headers=headers)
            data = r.content
            body = data
            goturl = r.url
        #while data and (len(body) <= MAX_SIZE):
        #    data = r.raw.read(CHUNK)
        #    body += data
        #expected_length = int(r.headers.get("Content-Length", 0))

        # If len(data) < CHUNK we're at the end of the file.
        # If it's the expected length, that's great we got the whole file,
        # even if len(data) == CHUNK.
        # Otherwise because we've read a full chunk of an unknown total
        # length we're not at the end of the file.
        #if not len(body) == expected_length and not (len(data) < CHUNK):
        #    print "file too big", url
    except eventlet.Timeout as t:
        if t is not timeout:
            raise  # not my timeout
            # print "took too long", url
    finally:
        timeout.cancel()

    return body, goturl


@task
def clamd_scan(url):
    try:
        clamd_socket = clamd.ClamdUnixSocket()
        body = fetch(url)[0]
        result = clamd_socket.scan_stream(body)
        if result:
            if "FOUND" in str(result):
                print result, url
                return 1
        return 0
    except RequestException:
        pass
    except Exception as e:
        print e.message
    return 0.5

@task
def tmax(iterable):
    return [{
        "type": "generic",
        "confidence": max(iterable)
    }]


@task
def crawl_html(url):
    try:
        callbacks = crawl_html.request.callbacks
        crawl_html.request.callbacks = []
        """
        Naive html crawler.

        To be truly effective we need to determine what files might actually effect
        the rendering of the page and what files a user might be tricked into
        downloading.

        * JavaScript in <script> tags
        * Executable files

        Things we might not be interested in:
        * images
        * links to other domains (example.com and example.net differ, but www.example.com and example.com do not) that are pure HTML
        * CSS

        Also not we need to be careful with how much we download!

        We don't want to sit and download an 8GB DVD or even an unlimited byte radio
        stream!

        We also want to avoid downloading the same page twice (once to get the links
        then again to scan the page itself).
        """
        body, rurl = fetch(url)
        html = lxml.html.soupparser.fromstring(
            body
        )
        html.make_links_absolute(
            resolve_base_href=True,
            base_url=rurl
        )
        links = {link for element, attribute, link, pos in html.iterlinks()}
        urls = []
        for link in links:
            if urlparse(link).scheme in {"http", "https", "ftp"}:
                urls.append(link)
        header = [clamd_scan.s(url) for url in urls]

        if callbacks:
            chord(
                header,
                (tmax.s() | group(callbacks))
            )()
    except:
        return [{
            "type": "generic",
            "confidence": 0.5
        }]


if __name__ == "__main__":
    print clamd_scan("https://secure.eicar.org/eicar.com")
    print crawl_html("http://www.eicar.org/85-0-Download.html")
